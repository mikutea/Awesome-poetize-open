"""
POETIZEæ–‡ç« çŸ¥è¯†åº“ MCP æœåŠ¡å™¨
æä¾›æ–‡ç« æœç´¢ã€æ‘˜è¦ã€åˆ†å—ç­‰åŠŸèƒ½

ä¸»è¦åŠŸèƒ½ï¼š
- æ–‡ç« æœç´¢
- æ–‡ç« æ‘˜è¦
- æ–‡ç« åˆ†å—
- æ–‡ç« å¯¹æ¯”
- æ–‡ç« èµ„æºè®¿é—®
- æ–‡ç« ç»Ÿè®¡
"""
import asyncio
import json
import logging
import sys
import os
from typing import List
from fastmcp import FastMCP, Context
import httpx
from config import JAVA_BACKEND_URL

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ä»ç¯å¢ƒå˜é‡æˆ–é…ç½®æ–‡ä»¶è¯»å–åç«¯URL
BACKEND_URL = JAVA_BACKEND_URL

# åˆ›å»ºFastMCPå®ä¾‹
mcp = FastMCP("POETIZEæ–‡ç« çŸ¥è¯†åº“")


def chunk_text(text: str, chunk_size: int = 2000, overlap: int = 200) -> List[str]:
    """
    å°†é•¿æ–‡æœ¬åˆ†å‰²æˆé‡å çš„chunks
    
    Args:
        text: è¦åˆ†å‰²çš„æ–‡æœ¬
        chunk_size: æ¯ä¸ªchunkçš„å¤§å°ï¼ˆå­—ç¬¦æ•°ï¼‰
        overlap: chunkä¹‹é—´çš„é‡å éƒ¨åˆ†
    
    Returns:
        List[str]: åˆ†å‰²åçš„æ–‡æœ¬å—åˆ—è¡¨
    """
    if len(text) <= chunk_size:
        return [text]
    
    chunks = []
    start = 0
    
    while start < len(text):
        end = start + chunk_size
        
        # å¦‚æœä¸æ˜¯æœ€åä¸€ä¸ªchunkï¼Œå°è¯•åœ¨å¥å­è¾¹ç•Œå¤„åˆ†å‰²
        if end < len(text):
            # å¯»æ‰¾æœ€è¿‘çš„å¥å­ç»“æŸç¬¦
            for delimiter in ['ã€‚', 'ï¼', 'ï¼Ÿ', '\n\n', '\n', 'ï¼Œ', ' ']:
                pos = text.rfind(delimiter, start, end)
                if pos != -1:
                    end = pos + 1
                    break
        
        chunks.append(text[start:end].strip())
        start = end - overlap if end < len(text) else end
    
    return chunks


def extract_summary(content: str, max_length: int = 500) -> str:
    """
    ä»æ–‡ç« å†…å®¹ä¸­æå–æ‘˜è¦
    ä¼˜å…ˆä½¿ç”¨å‰å‡ æ®µï¼Œå¦‚æœå¤ªçŸ­åˆ™ä½¿ç”¨å¼€å¤´éƒ¨åˆ†
    """
    # æŒ‰æ®µè½åˆ†å‰²
    paragraphs = [p.strip() for p in content.split('\n\n') if p.strip()]
    
    if not paragraphs:
        return content[:max_length]
    
    summary = []
    current_length = 0
    
    for para in paragraphs[:5]:  # æœ€å¤šå–å‰5æ®µ
        if current_length + len(para) > max_length:
            break
        summary.append(para)
        current_length += len(para)
    
    result = '\n\n'.join(summary)
    
    if len(result) < 100 and content:  # å¦‚æœå¤ªçŸ­ï¼Œç›´æ¥æˆªå–å¼€å¤´
        result = content[:max_length]
    
    return result


@mcp.tool()
async def search_and_summarize(keyword: str, max_articles: int = 5, ctx: Context = None) -> str:
    """
    æ™ºèƒ½æœç´¢å¹¶æ€»ç»“æ–‡ç« ï¼ˆRAGå¢å¼ºç‰ˆï¼‰
    
    è¿™ä¸ªå·¥å…·ä¼šï¼š
    1. æœç´¢ç›¸å…³æ–‡ç« 
    2. è·å–æ–‡ç« æ‘˜è¦
    3. è¿”å›ç»“æ„åŒ–çš„æœç´¢ç»“æœï¼Œä¾¿äºAIè¿›è¡Œæ€»ç»“
    
    Args:
        keyword: æœç´¢å…³é”®è¯
        max_articles: æœ€å¤šè¿”å›çš„æ–‡ç« æ•°é‡ï¼Œé»˜è®¤5ç¯‡
        ctx: MCPä¸Šä¸‹æ–‡ï¼ˆè‡ªåŠ¨æ³¨å…¥ï¼‰
    
    Returns:
        str: ç»“æ„åŒ–çš„æ–‡ç« æ‘˜è¦ä¿¡æ¯
    """
    
    # ä½¿ç”¨Contextè¿›è¡Œè¿›åº¦æŠ¥å‘Š
    if ctx:
        await ctx.info(f"æ­£åœ¨æœç´¢å…³äºã€Œ{keyword}ã€çš„æ–‡ç« ...")
    
    try:
        async with httpx.AsyncClient(timeout=15.0) as client:
            # æ™ºèƒ½å…³é”®è¯å¤„ç†ï¼šå°è¯•å¤šç§å˜ä½“
            import re
            search_keywords = [keyword]
            
            # å¦‚æœå…³é”®è¯ä¸­æ²¡æœ‰ç©ºæ ¼ï¼Œå°è¯•æ·»åŠ ç©ºæ ¼çš„ç‰ˆæœ¬ï¼ˆä¾‹å¦‚ï¼šVue3 -> Vue 3ï¼‰
            if re.match(r'^[A-Za-z]+\d+$', keyword):
                keyword_with_space = re.sub(r'([A-Za-z]+)(\d+)', r'\1 \2', keyword)
                search_keywords.append(keyword_with_space)
            
            # å¦‚æœå…³é”®è¯æœ‰ç©ºæ ¼ï¼Œä¹Ÿå°è¯•æ— ç©ºæ ¼çš„ç‰ˆæœ¬
            if ' ' in keyword:
                keyword_without_space = keyword.replace(' ', '')
                search_keywords.append(keyword_without_space)
                
                # å°†ç©ºæ ¼åˆ†éš”çš„å…³é”®è¯æ‹†åˆ†æˆç‹¬ç«‹çš„è¯è¿›è¡Œæœç´¢
                # ä¾‹å¦‚ï¼š"Vue React" -> ["Vue", "React"]
                split_keywords = [k.strip() for k in keyword.split() if k.strip()]
                if len(split_keywords) > 1:
                    search_keywords.extend(split_keywords)
            
            # å°è¯•å„ä¸ªå…³é”®è¯å˜ä½“ï¼Œæ”¶é›†ç»“æœ
            all_articles = []
            seen_ids = set()
            
            for search_key in search_keywords:
                request_body = {
                    "current": 1,
                    "size": max_articles,
                    "searchKey": search_key
                }
                
                response = await client.post(
                    f"{BACKEND_URL}/article/listArticle",
                    json=request_body
                )
                
                if response.status_code != 200:
                    logger.warning(f"æœç´¢å…³é”®è¯ '{search_key}' å¤±è´¥: HTTP {response.status_code}")
                    continue
                
                data = response.json()
                
                if data.get("code") != 200:
                    logger.warning(f"æœç´¢å…³é”®è¯ '{search_key}' å¤±è´¥: {data.get('message')}")
                    continue
                
                page_data = data.get("data", {})
                articles_batch = page_data.get("records", [])
                
                # åˆå¹¶ç»“æœï¼Œå»é‡
                for article in articles_batch:
                    article_id = article.get('id')
                    if article_id not in seen_ids:
                        seen_ids.add(article_id)
                        all_articles.append(article)
                
                # å¦‚æœå·²ç»æ‰¾åˆ°è¶³å¤Ÿçš„æ–‡ç« ï¼Œæå‰é€€å‡º
                if len(all_articles) >= max_articles:
                    break
            
            # ä½¿ç”¨åˆå¹¶åçš„ç»“æœ
            articles = all_articles[:max_articles]
            
            if not articles:
                return f"æœªæ‰¾åˆ°å…³äºã€Œ{keyword}ã€çš„æ–‡ç« ã€‚"
            
            if ctx:
                await ctx.info(f"æ‰¾åˆ° {len(articles)} ç¯‡ç›¸å…³æ–‡ç« ï¼Œæ­£åœ¨è·å–å†…å®¹...")
            
            # è·å–æ¯ç¯‡æ–‡ç« çš„è¯¦ç»†å†…å®¹
            article_summaries = []
            
            for i, article in enumerate(articles, 1):
                article_id = article.get('id')
                title = article.get('articleTitle', 'æ— æ ‡é¢˜')
                
                if ctx:
                    await ctx.info(f"æ­£åœ¨å¤„ç†ç¬¬ {i}/{len(articles)} ç¯‡: {title}")
                
                try:
                    # è·å–æ–‡ç« è¯¦æƒ…
                    detail_response = await client.get(
                        f"{BACKEND_URL}/article/getArticleById",
                        params={"id": article_id}
                    )
                    
                    if detail_response.status_code == 200:
                        detail_data = detail_response.json()
                        
                        if detail_data.get("code") == 200:
                            article_detail = detail_data.get("data", {})
                            content = article_detail.get('articleContent', '')
                            
                            # æå–æ‘˜è¦ï¼ˆé¿å…å†…å®¹å¤ªé•¿ï¼‰
                            summary = extract_summary(content, max_length=800)
                            
                            article_summaries.append({
                                'id': article_id,
                                'title': title,
                                'sort_name': article.get('sortName', 'æœªåˆ†ç±»'),
                                'label_name': article.get('labelName', 'æ— æ ‡ç­¾'),
                                'view_count': article.get('viewCount', 0),
                                'like_count': article.get('likeCount', 0),
                                'article_url': article.get('articleUrl', ''),
                                'summary': summary,
                                'content_length': len(content)
                            })
                
                except Exception as e:
                    logger.warning(f"è·å–æ–‡ç«  {article_id} å¤±è´¥: {e}")
                    continue
            
            if not article_summaries:
                return "âŒ æ— æ³•è·å–æ–‡ç« å†…å®¹"
            
            # æ„å»ºç»“æ„åŒ–çš„è¿”å›ç»“æœ
            result = f"ğŸ” å…³äºã€Œ{keyword}ã€çš„æœç´¢ç»“æœï¼ˆå…± {len(article_summaries)} ç¯‡ï¼‰ï¼š\n\n"
            result += "=" * 60 + "\n\n"
            
            for i, article_info in enumerate(article_summaries, 1):
                result += f"ã€æ–‡ç«  {i}ã€‘**{article_info['title']}**\n"
                result += f"ğŸ“ åˆ†ç±»: {article_info['sort_name']} | ğŸ·ï¸ æ ‡ç­¾: {article_info['label_name']}\n"
                result += f"ğŸ‘€ æµè§ˆ: {article_info['view_count']} | â¤ï¸ ç‚¹èµ: {article_info['like_count']}\n"
                result += f"ğŸ“„ å†…å®¹é•¿åº¦: {article_info['content_length']} å­—ç¬¦\n"
                result += f"ğŸ†” ID: {article_info['id']}\n"
                if article_info.get('article_url'):
                    result += f"ğŸ”— é“¾æ¥: {article_info['article_url']}\n"
                result += "\n"
                result += "**å†…å®¹æ‘˜è¦ï¼š**\n"
                result += article_info['summary']
                result += "\n\n" + "-" * 60 + "\n\n"
            
            result += "ğŸ’¡ æç¤ºï¼šä»¥ä¸Šæ˜¯æ–‡ç« çš„å…³é”®æ‘˜è¦ï¼Œä½ å¯ä»¥åŸºäºè¿™äº›å†…å®¹è¿›è¡Œæ€»ç»“å’Œåˆ†æã€‚\n"
            result += "å¦‚éœ€æŸ¥çœ‹å®Œæ•´å†…å®¹ï¼Œä½¿ç”¨ get_article_with_chunks(article_id) å·¥å…·ã€‚"
            
            if ctx:
                await ctx.info("âœ… å†…å®¹å‡†å¤‡å®Œæˆï¼Œå¯ä»¥å¼€å§‹æ€»ç»“äº†")
            
            return result
            
    except httpx.TimeoutException:
        return "âŒ è¯·æ±‚è¶…æ—¶ï¼Œè¯·ç¨åé‡è¯•"
    except Exception as e:
        logger.error(f"æ™ºèƒ½æœç´¢å¤±è´¥: {e}")
        return f"âŒ æœç´¢å¤±è´¥: {str(e)}"


@mcp.tool()
async def get_article_with_chunks(article_id: int, chunk_size: int = 2000) -> str:
    """
    è·å–æ–‡ç« å†…å®¹å¹¶åˆ†å—è¿”å›ï¼ˆé€‚åˆé•¿æ–‡ç« ï¼‰
    
    å°†é•¿æ–‡ç« åˆ†å‰²æˆå¤šä¸ªé‡å çš„chunksï¼Œé¿å…è¶…å‡ºAIä¸Šä¸‹æ–‡é™åˆ¶
    
    Args:
        article_id: æ–‡ç« ID
        chunk_size: æ¯ä¸ªchunkçš„å¤§å°ï¼ˆå­—ç¬¦æ•°ï¼‰ï¼Œé»˜è®¤2000
    
    Returns:
        str: åˆ†å—åçš„æ–‡ç« å†…å®¹
    """
    
    try:
        async with httpx.AsyncClient(timeout=15.0) as client:
            response = await client.get(
                f"{BACKEND_URL}/article/getArticleById",
                params={"id": article_id}
            )
            
            if response.status_code != 200:
                return f"âŒ è·å–æ–‡ç« å¤±è´¥: HTTP {response.status_code}"
            
            data = response.json()
            
            if data.get("code") != 200:
                error_msg = data.get("message", "æœªçŸ¥é”™è¯¯")
                return f"âŒ è·å–æ–‡ç« å¤±è´¥: {error_msg}"
            
            article = data.get("data", {})
            
            if not article:
                return f"âŒ æœªæ‰¾åˆ°IDä¸º {article_id} çš„æ–‡ç« "
            
            title = article.get('articleTitle', 'æ— æ ‡é¢˜')
            content = article.get('articleContent', 'æ— å†…å®¹')
            sort_name = article.get('sortName', 'æœªåˆ†ç±»')
            article_url = article.get('articleUrl', '')
            
            # ä¼˜å…ˆä½¿ç”¨labelNameå­—æ®µï¼Œå¦‚æœæ²¡æœ‰åˆ™å°è¯•ä»labelListæˆ–labelå¯¹è±¡è·å–
            label_name = article.get('labelName')
            if not label_name:
                # å°è¯•ä»labelå¯¹è±¡è·å–
                label = article.get('label', {})
                if label:
                    label_name = label.get('labelName')
            if not label_name:
                # å°è¯•ä»labelListè·å–
                label_list = article.get('labelList', [])
                if label_list:
                    label_name = ', '.join([label.get('labelName', '') for label in label_list])
            labels = label_name if label_name else 'æ— æ ‡ç­¾'
            
            # åˆ†å—å¤„ç†
            chunks = chunk_text(content, chunk_size=chunk_size)
            
            result = f"""# {title}

---
**åˆ†ç±»**: {sort_name} | **æ ‡ç­¾**: {labels}
**æ–‡ç« ID**: {article_id}
**å†…å®¹é•¿åº¦**: {len(content)} å­—ç¬¦
**åˆ†å—æ•°é‡**: {len(chunks)} å—"""
            if article_url:
                result += f"\n**æ–‡ç« é“¾æ¥**: {article_url}"
            result += "\n---\n\n"
            
            if len(chunks) == 1:
                result += content
            else:
                result += f"ğŸ“š **æ³¨æ„**: æ­¤æ–‡ç« è¾ƒé•¿ï¼Œå·²åˆ†ä¸º {len(chunks)} ä¸ªéƒ¨åˆ†\n\n"
                
                for i, chunk in enumerate(chunks, 1):
                    result += f"### ã€ç¬¬ {i}/{len(chunks)} éƒ¨åˆ†ã€‘\n\n"
                    result += chunk
                    result += f"\n\n{'=' * 60}\n\n"
            
            return result
            
    except httpx.TimeoutException:
        return "âŒ è¯·æ±‚è¶…æ—¶ï¼Œè¯·ç¨åé‡è¯•"
    except Exception as e:
        logger.error(f"è·å–æ–‡ç« å†…å®¹å¤±è´¥: {e}")
        return f"âŒ è·å–å¤±è´¥: {str(e)}"


@mcp.tool()
async def search_articles(keyword: str, limit: int = 10) -> str:
    """æœç´¢ç½‘ç«™æ–‡ç« ï¼ˆåŸºç¡€ç‰ˆï¼‰
    
    è¿”å›åŒ¹é…çš„æ–‡ç« åˆ—è¡¨ï¼ŒåŒ…å«æ ‡é¢˜ã€åˆ†ç±»ã€æ ‡ç­¾ç­‰åŸºæœ¬ä¿¡æ¯ã€‚
    
    Args:
        keyword: æœç´¢å…³é”®è¯
        limit: è¿”å›ç»“æœæ•°é‡é™åˆ¶ï¼Œé»˜è®¤10ç¯‡
    """
    
    try:
        async with httpx.AsyncClient(timeout=10.0) as client:
            # æ™ºèƒ½å…³é”®è¯å¤„ç†ï¼šå°è¯•å¤šç§å˜ä½“
            import re
            search_keywords = [keyword]
            
            # å¦‚æœå…³é”®è¯ä¸­æ²¡æœ‰ç©ºæ ¼ï¼Œå°è¯•æ·»åŠ ç©ºæ ¼çš„ç‰ˆæœ¬ï¼ˆä¾‹å¦‚ï¼šVue3 -> Vue 3ï¼‰
            # åŒ¹é…å­—æ¯+æ•°å­—çš„ç»„åˆï¼ˆå¦‚Vue3, React18ç­‰ï¼‰
            if re.match(r'^[A-Za-z]+\d+$', keyword):
                # åœ¨å­—æ¯å’Œæ•°å­—ä¹‹é—´æ’å…¥ç©ºæ ¼
                keyword_with_space = re.sub(r'([A-Za-z]+)(\d+)', r'\1 \2', keyword)
                search_keywords.append(keyword_with_space)
            
            # å¦‚æœå…³é”®è¯æœ‰ç©ºæ ¼ï¼Œä¹Ÿå°è¯•æ— ç©ºæ ¼çš„ç‰ˆæœ¬
            if ' ' in keyword:
                keyword_without_space = keyword.replace(' ', '')
                search_keywords.append(keyword_without_space)
                
                # å°†ç©ºæ ¼åˆ†éš”çš„å…³é”®è¯æ‹†åˆ†æˆç‹¬ç«‹çš„è¯è¿›è¡Œæœç´¢
                # ä¾‹å¦‚ï¼š"Vue React" -> ["Vue", "React"]
                split_keywords = [k.strip() for k in keyword.split() if k.strip()]
                if len(split_keywords) > 1:
                    search_keywords.extend(split_keywords)
            
            # å°è¯•å„ä¸ªå…³é”®è¯å˜ä½“
            all_articles = []
            seen_ids = set()
            
            for search_key in search_keywords:
                request_body = {
                    "current": 1,
                    "size": limit,
                    "searchKey": search_key
                }
                
                response = await client.post(
                    f"{BACKEND_URL}/article/listArticle",
                    json=request_body
                )
                
                if response.status_code != 200:
                    logger.warning(f"æœç´¢å…³é”®è¯ '{search_key}' å¤±è´¥: HTTP {response.status_code}")
                    continue
                
                data = response.json()
                
                if data.get("code") != 200:
                    logger.warning(f"æœç´¢å…³é”®è¯ '{search_key}' å¤±è´¥: {data.get('message')}")
                    continue
                
                page_data = data.get("data", {})
                articles = page_data.get("records", [])
                
                # åˆå¹¶ç»“æœï¼Œå»é‡
                for article in articles:
                    article_id = article.get('id')
                    if article_id not in seen_ids:
                        seen_ids.add(article_id)
                        all_articles.append(article)
                
                # å¦‚æœå·²ç»æ‰¾åˆ°è¶³å¤Ÿçš„æ–‡ç« ï¼Œæå‰é€€å‡º
                if len(all_articles) >= limit:
                    break
            
            # ä½¿ç”¨åˆå¹¶åçš„ç»“æœ
            articles = all_articles[:limit]
            
            if not articles:
                return f"æœªæ‰¾åˆ°å…³äºã€Œ{keyword}ã€çš„æ–‡ç« ã€‚\n\nğŸ’¡ æç¤ºï¼šè¯•è¯•å…¶ä»–å…³é”®è¯æˆ–æŸ¥çœ‹æ‰€æœ‰åˆ†ç±»ã€‚"
            
            # ä¼˜å…ˆä½¿ç”¨recordsçš„é•¿åº¦ï¼Œå¦‚æœtotalä¸º0ä½†æœ‰è®°å½•ï¼Œä½¿ç”¨å®é™…è®°å½•æ•°
            total = page_data.get("total", 0)
            if total == 0 and len(articles) > 0:
                total = len(articles)
            
            result = f"æ‰¾åˆ° {total} ç¯‡å…³äºã€Œ{keyword}ã€çš„æ–‡ç« ï¼ˆæ˜¾ç¤ºå‰ {len(articles)} ç¯‡ï¼‰ï¼š\n\n"
            
            for i, article in enumerate(articles, 1):
                title = article.get('articleTitle', 'æ— æ ‡é¢˜')
                article_id = article.get('id', '')
                sort_name = article.get('sortName', 'æœªåˆ†ç±»')
                label_name = article.get('labelName', 'æ— æ ‡ç­¾')
                view_count = article.get('viewCount', 0)
                like_count = article.get('likeCount', 0)
                article_url = article.get('articleUrl', '')
                
                result += f"{i}. **{title}**\n"
                result += f"   ğŸ“ åˆ†ç±»: {sort_name}\n"
                result += f"   ğŸ·ï¸ æ ‡ç­¾: {label_name}\n"
                result += f"   ğŸ‘€ æµè§ˆ: {view_count} | â¤ï¸ ç‚¹èµ: {like_count}\n"
                result += f"   ğŸ†” ID: {article_id}\n"
                if article_url:
                    result += f"   ğŸ”— é“¾æ¥: {article_url}\n"
                result += "\n"
            
            result += "ğŸ’¡ ä½¿ç”¨ search_and_summarize() å¯ä»¥æ™ºèƒ½æœç´¢å¹¶è·å–æ–‡ç« æ‘˜è¦ï¼Œä¾¿äºæ€»ç»“\n"
            result += "ğŸ’¡ ä½¿ç”¨ get_article_with_chunks(article_id) æŸ¥çœ‹å®Œæ•´å†…å®¹ï¼ˆé•¿æ–‡ç« ä¼šè‡ªåŠ¨åˆ†å—ï¼‰"
            return result
            
    except Exception as e:
        logger.error(f"æœç´¢æ–‡ç« å¤±è´¥: {e}")
        return f"âŒ æœç´¢å¤±è´¥: {str(e)}"


@mcp.tool()
async def get_article_content(article_id: int) -> str:
    """è·å–æ–‡ç« å®Œæ•´å†…å®¹ï¼ˆæ ‡å‡†ç‰ˆï¼‰
    
    ç›´æ¥è¿”å›æ–‡ç« çš„å®Œæ•´å†…å®¹ï¼Œä¸åˆ†å—ã€‚é€‚åˆä¸­çŸ­ç¯‡æ–‡ç« ã€‚
    å¦‚æœæ–‡ç« å¾ˆé•¿ï¼Œå»ºè®®ä½¿ç”¨ get_article_with_chunks() å·¥å…·ã€‚
    
    Args:
        article_id: æ–‡ç« ID
    
    Returns:
        str: æ–‡ç« çš„å®Œæ•´Markdownæ ¼å¼å†…å®¹
    """
    
    try:
        async with httpx.AsyncClient(timeout=15.0) as client:
            response = await client.get(
                f"{BACKEND_URL}/article/getArticleById",
                params={"id": article_id}
            )
            
            if response.status_code != 200:
                return f"âŒ è·å–æ–‡ç« å¤±è´¥: HTTP {response.status_code}"
            
            data = response.json()
            
            if data.get("code") != 200:
                error_msg = data.get("message", "æœªçŸ¥é”™è¯¯")
                return f"âŒ è·å–æ–‡ç« å¤±è´¥: {error_msg}"
            
            article = data.get("data", {})
            
            if not article:
                return f"âŒ æœªæ‰¾åˆ°IDä¸º {article_id} çš„æ–‡ç« "
            
            title = article.get('articleTitle', 'æ— æ ‡é¢˜')
            content = article.get('articleContent', 'æ— å†…å®¹')
            sort_name = article.get('sortName', 'æœªåˆ†ç±»')
            article_url = article.get('articleUrl', '')
            
            # ä¼˜å…ˆä½¿ç”¨labelNameå­—æ®µï¼Œå¦‚æœæ²¡æœ‰åˆ™å°è¯•ä»labelListæˆ–labelå¯¹è±¡è·å–
            label_name = article.get('labelName')
            if not label_name:
                # å°è¯•ä»labelå¯¹è±¡è·å–
                label = article.get('label', {})
                if label:
                    label_name = label.get('labelName')
            if not label_name:
                # å°è¯•ä»labelListè·å–
                label_list = article.get('labelList', [])
                if label_list:
                    label_name = ', '.join([label.get('labelName', '') for label in label_list])
            labels = label_name if label_name else 'æ— æ ‡ç­¾'
            
            view_count = article.get('viewCount', 0)
            like_count = article.get('likeCount', 0)
            create_time = article.get('createTime', '')
            
            result = f"""# {title}

---
**åˆ†ç±»**: {sort_name}
**æ ‡ç­¾**: {labels}
**å‘å¸ƒæ—¶é—´**: {create_time}
**æµè§ˆ**: {view_count} | **ç‚¹èµ**: {like_count}"""
            if article_url:
                result += f"\n**æ–‡ç« é“¾æ¥**: {article_url}"
            result += f"""
---

{content}

---
æ–‡ç« ID: {article_id}
"""
            return result
            
    except httpx.TimeoutException:
        return "âŒ è¯·æ±‚è¶…æ—¶ï¼Œè¯·ç¨åé‡è¯•"
    except Exception as e:
        logger.error(f"è·å–æ–‡ç« å†…å®¹å¤±è´¥: {e}")
        return f"âŒ è·å–å¤±è´¥: {str(e)}"


@mcp.tool()
async def list_categories() -> str:
    """åˆ—å‡ºæ‰€æœ‰æ–‡ç« åˆ†ç±»
    
    è·å–ç½‘ç«™æ‰€æœ‰æ–‡ç« åˆ†ç±»åŠæ¯ä¸ªåˆ†ç±»ä¸‹çš„æ–‡ç« æ•°é‡ã€‚
    
    Returns:
        str: æ ¼å¼åŒ–çš„åˆ†ç±»åˆ—è¡¨
    """
    
    try:
        async with httpx.AsyncClient(timeout=10.0) as client:
            response = await client.get(f"{BACKEND_URL}/webInfo/getSortInfo")
            
            if response.status_code != 200:
                return f"âŒ è·å–åˆ†ç±»å¤±è´¥: HTTP {response.status_code}"
            
            data = response.json()
            
            if data.get("code") != 200:
                error_msg = data.get("message", "æœªçŸ¥é”™è¯¯")
                return f"âŒ è·å–åˆ†ç±»å¤±è´¥: {error_msg}"
            
            categories = data.get("data", [])
            
            if not categories:
                return "æš‚æ— åˆ†ç±»"
            
            result = "ğŸ“ æ–‡ç« åˆ†ç±»åˆ—è¡¨ï¼š\n\n"
            
            for i, cat in enumerate(categories, 1):
                sort_name = cat.get('sortName', 'æœªå‘½å')
                sort_description = cat.get('sortDescription', '')
                count = cat.get('countOfSort', 0)
                sort_id = cat.get('id', '')
                
                result += f"{i}. **{sort_name}** ({count}ç¯‡)\n"
                if sort_description:
                    result += f"   ğŸ“ {sort_description}\n"
                result += f"   ğŸ†” ID: {sort_id}\n\n"
            
            result += "ğŸ’¡ ä½¿ç”¨ get_articles_by_category(category_id) æŸ¥çœ‹åˆ†ç±»ä¸‹çš„æ–‡ç« "
            return result
            
    except Exception as e:
        logger.error(f"è·å–åˆ†ç±»å¤±è´¥: {e}")
        return f"âŒ è·å–å¤±è´¥: {str(e)}"


@mcp.tool()
async def get_articles_by_category(category_id: int, page: int = 1, page_size: int = 10) -> str:
    """æ ¹æ®åˆ†ç±»è·å–æ–‡ç« åˆ—è¡¨
    
    Args:
        category_id: åˆ†ç±»ID
        page: é¡µç ï¼Œé»˜è®¤ç¬¬1é¡µ
        page_size: æ¯é¡µæ–‡ç« æ•°ï¼Œé»˜è®¤10ç¯‡
    
    Returns:
        str: è¯¥åˆ†ç±»ä¸‹çš„æ–‡ç« åˆ—è¡¨
    """
    
    try:
        async with httpx.AsyncClient(timeout=10.0) as client:
            response = await client.post(
                f"{BACKEND_URL}/article/listArticle",
                json={
                    "current": page,
                    "size": page_size,
                    "sortId": category_id
                }
            )
            
            if response.status_code != 200:
                return f"âŒ è·å–æ–‡ç« å¤±è´¥: HTTP {response.status_code}"
            
            data = response.json()
            
            if data.get("code") != 200:
                error_msg = data.get("message", "æœªçŸ¥é”™è¯¯")
                return f"âŒ è·å–æ–‡ç« å¤±è´¥: {error_msg}"
            
            page_data = data.get("data", {})
            records = page_data.get("records", [])
            total = page_data.get("total", 0)
            pages = page_data.get("pages", 1)
            
            if not records:
                return f"è¯¥åˆ†ç±»æš‚æ— æ–‡ç« "
            
            result = f"ğŸ“ åˆ†ç±»æ–‡ç« åˆ—è¡¨ï¼ˆç¬¬ {page}/{pages} é¡µï¼Œå…± {total} ç¯‡ï¼‰ï¼š\n\n"
            
            for i, article in enumerate(records, 1):
                title = article.get('articleTitle', 'æ— æ ‡é¢˜')
                article_id = article.get('id', '')
                view_count = article.get('viewCount', 0)
                like_count = article.get('likeCount', 0)
                create_time = article.get('createTime', '')
                
                result += f"{i}. **{title}**\n"
                result += f"   ğŸ‘€ {view_count} | â¤ï¸ {like_count} | ğŸ“… {create_time}\n"
                result += f"   ğŸ†” ID: {article_id}\n\n"
            
            if page < pages:
                result += f"\nğŸ’¡ ä½¿ç”¨ get_articles_by_category({category_id}, {page + 1}) æŸ¥çœ‹ä¸‹ä¸€é¡µ"
            
            return result
            
    except Exception as e:
        logger.error(f"è·å–åˆ†ç±»æ–‡ç« å¤±è´¥: {e}")
        return f"âŒ è·å–å¤±è´¥: {str(e)}"


@mcp.tool()
async def get_article_statistics() -> str:
    """è·å–ç½‘ç«™æ–‡ç« ç»Ÿè®¡ä¿¡æ¯
    
    Returns:
        str: æ–‡ç« æ•°é‡ã€è®¿é—®é‡ç­‰ç»Ÿè®¡ä¿¡æ¯
    """
    
    try:
        async with httpx.AsyncClient(timeout=10.0) as client:
            response = await client.get(f"{BACKEND_URL}/webInfo/getWebInfo")
            
            if response.status_code != 200:
                return f"âŒ è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: HTTP {response.status_code}"
            
            data = response.json()
            
            if data.get("code") != 200:
                error_msg = data.get("message", "æœªçŸ¥é”™è¯¯")
                return f"âŒ è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {error_msg}"
            
            web_info = data.get("data", {})
            
            web_name = web_info.get('webName', 'Poetize')
            web_title = web_info.get('webTitle', '')
            history_all_count = web_info.get('historyAllCount', '0')
            article_count = web_info.get('articleCount', 0)
            
            result = f"""ğŸ“Š **{web_name} ç½‘ç«™ç»Ÿè®¡**

- ç½‘ç«™æ ‡é¢˜: {web_title}
- æ–‡ç« æ€»æ•°: {article_count}ç¯‡
- æ€»è®¿é—®é‡: {history_all_count}

ğŸ’¡ ä½¿ç”¨ä»¥ä¸‹å·¥å…·æŸ¥çœ‹æ›´å¤šä¿¡æ¯ï¼š
- list_categories() - æŸ¥çœ‹æ‰€æœ‰åˆ†ç±»
- search_articles(keyword) - æœç´¢æ–‡ç« 
- get_hot_articles() - æŸ¥çœ‹çƒ­é—¨æ–‡ç« 
"""
            return result
            
    except Exception as e:
        logger.error(f"è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
        return f"âŒ è·å–å¤±è´¥: {str(e)}"


@mcp.tool()
async def get_hot_articles(limit: int = 10) -> str:
    """è·å–çƒ­é—¨æ–‡ç« åˆ—è¡¨
    
    æ ¹æ®æ™ºèƒ½çƒ­åº¦ç®—æ³•æ’åºï¼Œç»¼åˆè€ƒè™‘æµè§ˆé‡ã€ç‚¹èµæ•°ã€è¯„è®ºæ•°ç­‰å› ç´ ã€‚
    
    Args:
        limit: è¿”å›æ–‡ç« æ•°é‡ï¼Œé»˜è®¤10ç¯‡
    
    Returns:
        str: çƒ­é—¨æ–‡ç« åˆ—è¡¨
    """
    
    try:
        async with httpx.AsyncClient(timeout=10.0) as client:
            response = await client.get(f"{BACKEND_URL}/article/getHotArticles")
            
            if response.status_code != 200:
                return f"âŒ è·å–çƒ­é—¨æ–‡ç« å¤±è´¥: HTTP {response.status_code}"
            
            data = response.json()
            
            if data.get("code") != 200:
                error_msg = data.get("message", "æœªçŸ¥é”™è¯¯")
                return f"âŒ è·å–çƒ­é—¨æ–‡ç« å¤±è´¥: {error_msg}"
            
            articles = data.get("data", [])
            
            if not articles:
                return "æš‚æ— çƒ­é—¨æ–‡ç« "
            
            articles = articles[:limit]
            
            result = f"ğŸ”¥ çƒ­é—¨æ–‡ç« TOP {len(articles)}ï¼š\n\n"
            
            for i, article in enumerate(articles, 1):
                title = article.get('articleTitle', 'æ— æ ‡é¢˜')
                article_id = article.get('id', '')
                sort_name = article.get('sortName', 'æœªåˆ†ç±»')
                view_count = article.get('viewCount', 0)
                like_count = article.get('likeCount', 0)
                comment_count = article.get('commentCount', 0)
                article_url = article.get('articleUrl', '')
                
                result += f"{i}. **{title}**\n"
                result += f"   ğŸ“ {sort_name}\n"
                result += f"   ğŸ‘€ {view_count} | â¤ï¸ {like_count} | ğŸ’¬ {comment_count}\n"
                result += f"   ğŸ†” ID: {article_id}\n"
                if article_url:
                    result += f"   ğŸ”— é“¾æ¥: {article_url}\n"
                result += "\n"
            
            result += "ğŸ’¡ ä½¿ç”¨ get_article_content(article_id) æˆ– search_and_summarize() æŸ¥çœ‹è¯¦æƒ…"
            return result
            
    except Exception as e:
        logger.error(f"è·å–çƒ­é—¨æ–‡ç« å¤±è´¥: {e}")
        return f"âŒ è·å–å¤±è´¥: {str(e)}"


@mcp.tool()
async def compare_articles(article_ids: List[int], ctx: Context = None) -> str:
    """
    å¯¹æ¯”å¤šç¯‡æ–‡ç« çš„å†…å®¹
    
    è·å–å¤šç¯‡æ–‡ç« çš„æ‘˜è¦ï¼Œä¾¿äºAIè¿›è¡Œå¯¹æ¯”åˆ†æ
    
    Args:
        article_ids: è¦å¯¹æ¯”çš„æ–‡ç« IDåˆ—è¡¨ï¼ˆæœ€å¤š5ç¯‡ï¼‰
        ctx: MCPä¸Šä¸‹æ–‡ï¼ˆè‡ªåŠ¨æ³¨å…¥ï¼‰
    
    Returns:
        str: ç»“æ„åŒ–çš„å¯¹æ¯”ä¿¡æ¯
    """
    if len(article_ids) > 5:
        return "âŒ æœ€å¤šåªèƒ½å¯¹æ¯”5ç¯‡æ–‡ç« "
    
    if len(article_ids) < 2:
        return "âŒ è‡³å°‘éœ€è¦2ç¯‡æ–‡ç« æ‰èƒ½è¿›è¡Œå¯¹æ¯”"
    
    
    if ctx:
        await ctx.info(f"æ­£åœ¨è·å– {len(article_ids)} ç¯‡æ–‡ç« è¿›è¡Œå¯¹æ¯”...")
    
    try:
        async with httpx.AsyncClient(timeout=20.0) as client:
            articles_info = []
            
            for article_id in article_ids:
                try:
                    response = await client.get(
                        f"{BACKEND_URL}/article/getArticleById",
                        params={"id": article_id}
                    )
                    
                    if response.status_code == 200:
                        data = response.json()
                        
                        if data.get("code") == 200:
                            article = data.get("data", {})
                            content = article.get('articleContent', '')
                            
                            articles_info.append({
                                'id': article_id,
                                'title': article.get('articleTitle', 'æ— æ ‡é¢˜'),
                                'sort_name': article.get('sortName', 'æœªåˆ†ç±»'),
                                'view_count': article.get('viewCount', 0),
                                'like_count': article.get('likeCount', 0),
                                'create_time': article.get('createTime', ''),
                                'summary': extract_summary(content, max_length=600),
                                'content_length': len(content)
                            })
                
                except Exception as e:
                    logger.warning(f"è·å–æ–‡ç«  {article_id} å¤±è´¥: {e}")
                    continue
            
            if len(articles_info) < 2:
                return "âŒ æ— æ³•è·å–è¶³å¤Ÿçš„æ–‡ç« è¿›è¡Œå¯¹æ¯”"
            
            result = f"ğŸ“Š æ–‡ç« å¯¹æ¯”åˆ†æï¼ˆå…± {len(articles_info)} ç¯‡ï¼‰\n\n"
            result += "=" * 60 + "\n\n"
            
            for i, info in enumerate(articles_info, 1):
                result += f"### ã€æ–‡ç«  {i}ã€‘{info['title']}\n\n"
                result += f"- **åˆ†ç±»**: {info['sort_name']}\n"
                result += f"- **æµè§ˆ**: {info['view_count']} | **ç‚¹èµ**: {info['like_count']}\n"
                result += f"- **å‘å¸ƒæ—¶é—´**: {info['create_time']}\n"
                result += f"- **å†…å®¹é•¿åº¦**: {info['content_length']} å­—ç¬¦\n"
                result += f"- **ID**: {info['id']}\n\n"
                result += "**å†…å®¹æ‘˜è¦ï¼š**\n"
                result += info['summary']
                result += "\n\n" + "-" * 60 + "\n\n"
            
            result += "ğŸ’¡ æç¤ºï¼šä»¥ä¸Šæ˜¯å„ç¯‡æ–‡ç« çš„å…³é”®ä¿¡æ¯ï¼Œä½ å¯ä»¥åŸºäºè¿™äº›å†…å®¹è¿›è¡Œå¯¹æ¯”åˆ†æã€‚"
            
            if ctx:
                await ctx.info("âœ… å¯¹æ¯”å‡†å¤‡å®Œæˆ")
            
            return result
            
    except Exception as e:
        logger.error(f"å¯¹æ¯”æ–‡ç« å¤±è´¥: {e}")
        return f"âŒ å¯¹æ¯”å¤±è´¥: {str(e)}"


@mcp.resource("article://{article_id}")
async def article_resource(article_id: int) -> str:
    """é€šè¿‡èµ„æºæ–¹å¼è®¿é—®æ–‡ç« å†…å®¹
    
    è¿™æ˜¯ä¸€ä¸ªMCP Resourceï¼Œå¯ä»¥è¢«å®¢æˆ·ç«¯ç›´æ¥è¯»å–ã€‚
    """
    
    try:
        async with httpx.AsyncClient(timeout=15.0) as client:
            response = await client.get(
                f"{BACKEND_URL}/article/getArticleById",
                params={"id": article_id}
            )
            
            if response.status_code == 200:
                data = response.json()
                return json.dumps(data.get("data", {}), ensure_ascii=False, indent=2)
            
            return json.dumps({"error": f"HTTP {response.status_code}"})
            
    except Exception as e:
        return json.dumps({"error": str(e)})


if __name__ == "__main__":
    try:
        logger.info("æ­£åœ¨å¯åŠ¨æ–‡ç« çŸ¥è¯†åº“MCPæœåŠ¡å™¨...")
        
        # FastMCPæä¾›äº†åŒæ­¥çš„runæ–¹æ³•ï¼Œè‡ªåŠ¨å¤„ç†asyncioäº‹ä»¶å¾ªç¯
        mcp.run()
        
    except KeyboardInterrupt:
        logger.info("æœåŠ¡å™¨å·²åœæ­¢")
    except Exception as e:
        logger.error(f"å¯åŠ¨å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
